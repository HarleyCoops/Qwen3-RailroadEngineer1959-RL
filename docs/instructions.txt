We first want to develop a pipeline for extracting understanding from original Arxiv papers TeX source code. One example is in C:\Users\admin\Qwen2.5VL\OriginalPaper.

We have extensive APIs available for tool use and tool building. Some we may get an API error on and need to update.

For this project we want to connect through Cline using my existing OpenRouter connection to the latest Qwen3-VL Thinking model.

This will be a learning tool for both the model itself and for running and connecting to the model. I will use my existing Hugging Face profile (and optionally Inference Endpoints) to connect to and run this model.

I am particularly interested in understanding how to fine tune this model and what kind of dataset would be needed to fine tune a visual model. That is an advanced topic though. A future focus.

We will rely heavily on Cline and also want to build MCP servers to accomplish some of these tasks.

Think creatively about solutions where the most efficient option is not always the best. We want to explore tool use and MCP tool building while we also understand the academic paper for this model and the latest model card:

https://huggingface.co/Qwen/Qwen3-VL-235B-A22B-Thinking

```python
# Minimal local example (requires serious GPU resources)
import os
import torch
from transformers import AutoProcessor, Qwen3VLMoeForConditionalGeneration

model_id = "Qwen/Qwen3-VL-235B-A22B-Thinking"
processor = AutoProcessor.from_pretrained(model_id, token=os.environ.get("HF_API_KEY"))
model = Qwen3VLMoeForConditionalGeneration.from_pretrained(
    model_id,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    token=os.environ.get("HF_API_KEY"),
)

messages = [
    {
        "role": "user",
        "content": [{"type": "text", "text": "Who are you?"}],
    }
]
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=True,
    return_tensors="pt",
    return_dict=True,
)
inputs = {k: v.to(model.device) for k, v in inputs.items()}
outputs = model.generate(**inputs, max_new_tokens=256)
prompt_len = inputs["input_ids"].shape[1]
print(processor.batch_decode(outputs[:, prompt_len:], skip_special_tokens=True)[0])
```

Use your web browser tool to access these links:

@https://huggingface.co/docs/transformers/main/en/model_doc/qwen3_vl
@https://openrouter.ai/docs/use-cases/reasoning-tokens
@https://huggingface.co/docs/transformers/main_classes/pipelines

## Progress Documentation
For a detailed list of modules and remaining tasks, please refer to the PROGRESS.md file in the workspace root. This document includes a checklist of implemented components and features pending integration.

We want to pivot away from the Dakota extraction for a minute and focus on getting our IDE setup to work with OpenRouter for Qwen3-VL access, then we want to make sure we can connect to all the inference providers mentioned using the Hugging Face keys we have set up.
