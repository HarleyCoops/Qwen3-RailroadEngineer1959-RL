# Prime-RL Launch Guide - Quick Reference

## âœ… Setup Complete

Your Prime-RL training configuration is ready! Here's what's been set up:

### Config Files (Ready to Use)
- âœ… `train_30b.toml` - Trainer configuration
- âœ… `orch_30b.toml` - Orchestrator configuration  
- âœ… `infer_30b.toml` - Inference configuration

### Launch Script
- âœ… `launch_prime_rl_30b.ps1` - PowerShell launch script

## ğŸš€ How to Launch

### Option 1: Prime Intellect Platform (Recommended)

1. Go to https://app.primeintellect.ai
2. Create new RL training job
3. Upload the 3 config files from `dakota_rl_training/configs/`
4. Set GPU allocation: Inference (0-3), Trainer (4-7)
5. Launch!

### Option 2: Command Line (Linux/Multi-Node)

```bash
cd prime-rl-framework

uv run rl \
    --trainer @ ../dakota_rl_training/configs/train_30b.toml \
    --orchestrator @ ../dakota_rl_training/configs/orch_30b.toml \
    --inference @ ../dakota_rl_training/configs/infer_30b.toml \
    --trainer-gpu-ids 4,5,6,7 \
    --inference-gpu-ids 0,1,2,3 \
    --output-dir ../dakota_rl_training/outputs/grpo_30b
```

### Option 3: PowerShell Script (Windows â†’ Linux Instance)

```powershell
cd dakota_rl_training
.\launch_prime_rl_30b.ps1
```

## ğŸ“‹ Configuration Summary

| Setting | Value |
|---------|-------|
| Model | `qwen/qwen3-30b-a3b-instruct-2507` |
| Environment | `harleycooper/dakota1890` |
| Max Steps | 500 |
| Batch Size | 256 |
| Micro Batch Size | 2 |
| Sequence Length | 1536 |
| Rollouts per Example | 8 |
| Learning Rate | 1e-6 |
| Checkpoint Interval | Every 100 steps |
| W&B Project | `dakota-rl-grammar` |

## ğŸ“Š GPU Allocation (8 A100s)

- **Inference**: GPUs 0-3 (4 GPUs, dp=2, tp=2)
- **Trainer**: GPUs 4-7 (4 GPUs)

## ğŸ“ Output Structure

```
outputs/grpo_30b/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ step_100/
â”‚   â”œâ”€â”€ step_200/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ weights/
â”œâ”€â”€ rollouts/
â””â”€â”€ logs/
    â”œâ”€â”€ trainer/
    â”œâ”€â”€ orchestrator.log
    â””â”€â”€ inference.stdout
```

## ğŸ” Monitoring

- **W&B Dashboard**: https://wandb.ai/your-username/dakota-rl-grammar
- **Logs**: `outputs/grpo_30b/logs/`
- **Prime Intellect Dashboard**: https://app.primeintellect.ai

## â±ï¸ Expected Timeline

- Provisioning: 10-30 min
- Training: ~90 min
- Total: ~2 hours

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| Config file not found | Run from `prime-rl-framework` directory |
| Model not found | Verify: `qwen/qwen3-30b-a3b-instruct-2507` |
| Environment not found | Verify: `harleycooper/dakota1890` is published |
| Out of memory | Reduce `batch_size` in `orch_30b.toml` |
| CUDA error | `enforce_eager = true` already set in `infer_30b.toml` |

## ğŸ“š Documentation

- Full guide: `dakota_rl_training/LAUNCH_PRIME_RL.md`
- Prime-RL docs: https://github.com/PrimeIntellect-ai/prime-rl/tree/main/docs
- Environment: https://app.primeintellect.ai/dashboard/environments/harleycooper/dakota1890

## âœ… Pre-Launch Checklist

- [ ] All 3 config files exist
- [ ] Environment `harleycooper/dakota1890` is published
- [ ] Model `qwen/qwen3-30b-a3b-instruct-2507` is accessible
- [ ] W&B project `dakota-rl-grammar` exists (or will be created)
- [ ] Have access to 8 A100 GPUs (or equivalent)
- [ ] Output directory has sufficient disk space

---

**Ready to launch!** ğŸš€

For detailed instructions, see `LAUNCH_PRIME_RL.md`

