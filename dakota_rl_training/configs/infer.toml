# Dakota RL Inference Server Configuration
# For use with PrimeIntellect prime-rl vLLM backend

[model]
name_or_path = "Qwen/Qwen2.5-7B-Instruct"
revision = "main"
torch_dtype = "auto"

[server]
# vLLM inference server settings
host = "0.0.0.0"
port = 8000
max_concurrent_requests = 32
timeout = 300

[engine]
# vLLM engine configuration
tensor_parallel_size = 1  # Number of GPUs for tensor parallelism
pipeline_parallel_size = 1
max_model_len = 8192
gpu_memory_utilization = 0.90
swap_space = 4  # GB of CPU swap space
dtype = "auto"

[sampling]
# Default sampling parameters
temperature = 0.7
top_p = 0.9
max_tokens = 128
n = 1  # Number of completions per request
best_of = 1
presence_penalty = 0.0
frequency_penalty = 0.0

[tokenizer]
# Tokenizer settings
tokenizer_mode = "auto"
trust_remote_code = true

[logging]
log_level = "info"
log_requests = true
